# Imports
source("../../nn/layers/affine.dml") as affine
source("../../nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("../../nn/layers/dropout.dml") as dropout
source("../../nn/layers/relu.dml") as relu
source("../../nn/layers/softmax.dml") as softmax
source("../../nn/optim/sgd_nesterov.dml") as sgd_nesterov

X = read($X)
Y = read($Y)

W1 = read(""+$B+"/w1_nesterov_classify")
W2 = read(""+$B+"/w2_nesterov_classify")
W3 = read(""+$B+"/w3_nesterov_classify")
b1 = read(""+$B+"/b1_nesterov_classify")
b2 = read(""+$B+"/b2_nesterov_classify")
b3 = read(""+$B+"/b3_nesterov_classify")
p = read(""+$B+"/p_nesterov_classify")

# Compute forward pass with dropout
## layer 1:
out1 = affine::forward(X, W1, b1)
outr1 = relu::forward(out1)
[outd1, maskd1] = dropout::forward(outr1, p, -1)
## layer 2:
out2 = affine::forward(outd1, W2, b2)
outr2 = relu::forward(out2)
[outd2, maskd2] = dropout::forward(outr2, p, -1)
## layer 3:
out3 = affine::forward(outd2, W3, b3)
probs = softmax::forward(out3)

# Compute loss
loss = cross_entropy_loss::forward(probs, Y)
print("Cross entropy loss with dropout: " + loss)

# repeat without dropout
## layer 1:
out1 = affine::forward(X, W1, b1)
outr1 = relu::forward(out1)
## layer 2:
out2 = affine::forward(outr1, W2, b2)
outr2 = relu::forward(out2)
## layer 3:
out3 = affine::forward(outr2, W3, b3)
probs = softmax::forward(out3)

# Compute loss
loss = cross_entropy_loss::forward(probs, Y)
print("Cross entropy loss without dropout: " + loss)